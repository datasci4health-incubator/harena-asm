{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2020.10.28)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.51.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.9.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.1.94)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from protobuf->transformers) (49.6.0.post20201009)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.8/site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from torch) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97ec3850ad9476fbbe0c48cd888fe46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c376fed73946d2934cfa9b05b22d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ee68dc5ca040c1afc89a09230ecae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435780550.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# BERT_MODEL='bert-base-cased'\n",
    "BERT_MODEL='dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(BERT_MODEL, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model,tokenizer,marked_text):\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    \n",
    "    hidden_states = get_hidden_states(model, tokenizer, marked_text)\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the back, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes.\"\n",
    "tokenized_input = tokenizer.tokenize(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".        8\n",
       ",        7\n",
       "is       4\n",
       "to       4\n",
       "She      3\n",
       "        ..\n",
       "a        1\n",
       "heart    1\n",
       "##is     1\n",
       "first    1\n",
       "ER       1\n",
       "Length: 113, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# print(type(tokenized_input))\n",
    "# tokenized_input.remove(\"##\")\n",
    "# print(tokenized_input)\n",
    "\n",
    "tokenized_input_pd = pd.Index(tokenized_input)\n",
    "\n",
    "\n",
    "token_counts = tokenized_input_pd.value_counts()\n",
    "token_indexes = token_counts.keys()\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_embeddings = []\n",
    "last_embeddings = []\n",
    "\n",
    "with open(\"embeddings/label.tsv\", \"w\") as labels_file:\n",
    "    for token in tokenized_input:\n",
    "        if (not token.startswith(\"##\")):\n",
    "            marked_text = \"[CLS] \" + token + \" [SEP]\"\n",
    "#             print(type(token_counts[token]))\n",
    "            print(token + '\\t' + str(2), file=labels_file)\n",
    "\n",
    "            token_embeddings = get_embeddings(marked_text)\n",
    "        \n",
    "            first_embedding_layer = token_embeddings[1][1]\n",
    "            last_embedding_layer = token_embeddings[1][12]\n",
    "\n",
    "            tsv_rows = ''\n",
    "            for e in last_embedding_layer:\n",
    "                tsv_rows += str(e.item()) + '\\t'\n",
    "            last_embeddings.append(tsv_rows)\n",
    "            \n",
    "            tsv_rows = ''\n",
    "            for e in first_embedding_layer:\n",
    "                tsv_rows += str(e.item()) + '\\t'\n",
    "            first_embeddings.append(tsv_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "first_embeddings = []\n",
    "last_embeddings = []\n",
    "valid_tokens_indexes = []\n",
    "valid_tokens_counts = []\n",
    "# data = { token_indexes, token_counts }\n",
    "# print(data)\n",
    "# a = pd.Index(token_indexes, token_counts)\n",
    "# a.to_csv(\"embeddings/label.tsv\", header=[\"word\", \"count\"], index=False)\n",
    "\n",
    "\n",
    "\n",
    "# print(len(tokenized_input))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for token in tokenized_input:\n",
    "    if (not token.startswith(\"##\")):\n",
    "        i= i + 1\n",
    "        marked_text = \"[CLS] \" + token + \" [SEP]\"\n",
    "\n",
    "#             writer = csv.writer(labels_file)\n",
    "#             writer.writerow([token, token_counts[token]])\n",
    "\n",
    "#         token_indexes.append()\n",
    "        \n",
    "        valid_tokens_indexes.append(token)\n",
    "        valid_tokens_counts.append(token_counts[token])\n",
    "\n",
    "        token_embeddings = get_embeddings(marked_text)\n",
    "        \n",
    "        first_embedding_layer = token_embeddings[1][1]\n",
    "        last_embedding_layer = token_embeddings[1][12]\n",
    "\n",
    "        tsv_rows = ''\n",
    "        for e in last_embedding_layer:\n",
    "            tsv_rows += str(e.item()) + '\\t'\n",
    "        last_embeddings.append(tsv_rows)\n",
    "            \n",
    "        tsv_rows = ''\n",
    "        for e in first_embedding_layer:\n",
    "            tsv_rows += str(e.item()) + '\\t'\n",
    "        first_embeddings.append(tsv_rows)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings/biobert/first_embedding.tsv\", \"w\") as f:\n",
    "    for e in first_embeddings:\n",
    "        print (e, file=f)\n",
    "        \n",
    "with open(\"embeddings/biobert/last_embedding.tsv\", \"w\") as f:\n",
    "    for e in last_embeddings:\n",
    "        print (e, file=f)\n",
    "        \n",
    "# data = {'wotttrd':  valid_tokens_indexes,\n",
    "#         'count': valid_tokens_counts\n",
    "#        }\n",
    "# data_tuples = list(zip(valid_tokens_indexes,valid_tokens_counts))\n",
    "# df = pd.DataFrame(data_tuples)\n",
    "# df.to_csv(\"embeddings/label.tsv\", index=False, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1),\n",
       " ('58', 1),\n",
       " ('-', 3),\n",
       " ('year', 1),\n",
       " ('-', 3),\n",
       " ('old', 1),\n",
       " ('African', 1),\n",
       " ('-', 3),\n",
       " ('American', 1),\n",
       " ('woman', 1),\n",
       " ('presents', 1),\n",
       " ('to', 4),\n",
       " ('the', 3),\n",
       " ('ER', 1),\n",
       " ('with', 1),\n",
       " ('e', 1),\n",
       " ('pressing', 1),\n",
       " ('/', 1),\n",
       " ('burning', 1),\n",
       " ('anterior', 1),\n",
       " ('chest', 1),\n",
       " ('pain', 3),\n",
       " ('that', 1),\n",
       " ('began', 1),\n",
       " ('two', 1),\n",
       " ('days', 1),\n",
       " ('earlier', 1),\n",
       " ('for', 1),\n",
       " ('the', 3),\n",
       " ('first', 1),\n",
       " ('time', 1),\n",
       " ('in', 1),\n",
       " ('her', 2),\n",
       " ('life', 1),\n",
       " ('.', 8),\n",
       " ('The', 3),\n",
       " ('pain', 3),\n",
       " ('started', 1),\n",
       " ('while', 1),\n",
       " ('she', 1),\n",
       " ('was', 1),\n",
       " ('walking', 1),\n",
       " (',', 7),\n",
       " ('r', 1),\n",
       " ('to', 4),\n",
       " ('the', 3),\n",
       " ('back', 1),\n",
       " (',', 7),\n",
       " ('and', 3),\n",
       " ('is', 4),\n",
       " ('accompanied', 1),\n",
       " ('by', 1),\n",
       " ('nausea', 1),\n",
       " (',', 7),\n",
       " ('di', 1),\n",
       " ('and', 3),\n",
       " ('mild', 1),\n",
       " ('d', 1),\n",
       " (',', 7),\n",
       " ('but', 1),\n",
       " ('is', 4),\n",
       " ('not', 1),\n",
       " ('increased', 1),\n",
       " ('on', 1),\n",
       " ('inspiration', 1),\n",
       " ('.', 8),\n",
       " ('The', 3),\n",
       " ('latest', 1),\n",
       " ('episode', 1),\n",
       " ('of', 2),\n",
       " ('pain', 3),\n",
       " ('ended', 1),\n",
       " ('half', 1),\n",
       " ('an', 1),\n",
       " ('hour', 1),\n",
       " ('prior', 1),\n",
       " ('to', 4),\n",
       " ('her', 2),\n",
       " ('arrival', 1),\n",
       " ('.', 8),\n",
       " ('She', 3),\n",
       " ('is', 4),\n",
       " ('known', 1),\n",
       " ('to', 4),\n",
       " ('have', 1),\n",
       " ('h', 2),\n",
       " ('and', 3),\n",
       " ('o', 1),\n",
       " ('.', 8),\n",
       " ('She', 3),\n",
       " ('denies', 1),\n",
       " ('smoking', 1),\n",
       " (',', 7),\n",
       " ('diabetes', 1),\n",
       " (',', 7),\n",
       " ('h', 2),\n",
       " (',', 7),\n",
       " ('or', 1),\n",
       " ('a', 1),\n",
       " ('family', 1),\n",
       " ('history', 1),\n",
       " ('of', 2),\n",
       " ('heart', 1),\n",
       " ('disease', 1),\n",
       " ('.', 8),\n",
       " ('She', 3),\n",
       " ('currently', 1),\n",
       " ('takes', 1),\n",
       " ('no', 1),\n",
       " ('medications', 1),\n",
       " ('.', 8),\n",
       " ('Physical', 1),\n",
       " ('examination', 1),\n",
       " ('is', 4),\n",
       " ('normal', 1),\n",
       " ('.', 8),\n",
       " ('The', 3),\n",
       " ('E', 1),\n",
       " ('shows', 1),\n",
       " ('non', 1),\n",
       " ('changes', 1),\n",
       " ('.', 8)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
